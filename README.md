# Kubernetes Cluster Upgrade: v1.29 â†’ v1.30.14

[![Kubernetes](https://img.shields.io/badge/kubernetes-v1.30.14-326CE5?logo=kubernetes)](https://kubernetes.io/)
[![AWS](https://img.shields.io/badge/AWS-EC2-FF9900?logo=amazon-aws)](https://aws.amazon.com/ec2/)
[![Status](https://img.shields.io/badge/upgrade-success-brightgreen)](/)
[![Downtime](https://img.shields.io/badge/downtime-0%20seconds-blue)](/)

> A production-style Kubernetes cluster upgrade demonstrating zero-downtime operations, systematic troubleshooting, and comprehensive documentation practices.

## ğŸ¯ Project Overview

Successfully upgraded a 3-node Kubernetes cluster from version 1.29 to 1.30.14 with zero downtime, following official Kubernetes documentation and enterprise best practices.

**Key Achievements:**
- âœ… Zero-downtime rolling upgrade
- âœ… Multi-AZ AWS deployment
- âœ… Comprehensive documentation (735 lines of terminal capture)
- âœ… Automated state tracking using ConfigMaps
- âœ… Resolved complex AWS networking challenge

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AWS VPC (vpc-0bc53c73a3d679762)           â”‚
â”‚                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  AZ: us-east-1a  â”‚  â”‚  AZ: us-east-1b â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ Master     â”‚  â”‚  â”‚  â”‚ Worker 1   â”‚ â”‚ â”‚
â”‚  â”‚  â”‚ v1.30.14   â”‚â—„â”€â”¼â”€â”€â”¼â”€â–ºâ”‚ v1.30.14   â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚  â”‚ Worker 2   â”‚ â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚  â”‚ v1.30.14   â”‚ â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Infrastructure Details
- **Platform:** Self-managed Kubernetes on AWS EC2
- **Control Plane:** 1 master node
- **Workers:** 2 nodes across different availability zones
- **CNI:** Calico
- **Applications:** nginx, evershop e-commerce, PostgreSQL

## ğŸ“‹ Upgrade Process

### Phase 1: Control Plane (Master Node)
```bash
Version: 1.29.15 â†’ 1.30.14
Duration: ~30 minutes
Status: âœ… Success
```

1. Added Kubernetes v1.30 repository
2. Upgraded kubeadm to v1.30.14
3. Reviewed upgrade plan (`kubeadm upgrade plan`)
4. Applied control plane upgrade
5. Upgraded kubelet and kubectl
6. Verified all control plane components

### Phase 2: Worker Nodes
```bash
Worker 1: 1.29.6 â†’ 1.30.14 âœ…
Worker 2: 1.29.6 â†’ 1.30.14 âœ…
Duration: ~30 minutes each
```

Each worker upgraded using rolling deployment strategy:
- Drained node (workloads migrated automatically)
- Upgraded components
- Uncordoned node
- Verified health

## ğŸ”§ Technical Skills Demonstrated

### Kubernetes Administration
- Cluster upgrades using kubeadm
- Control plane component management
- Node lifecycle operations (cordon/drain/uncordon)
- Pod migration and rescheduling
- ConfigMap-based state management

### Cloud Infrastructure (AWS)
- Multi-AZ deployment architecture
- VPC networking configuration
- Security group management
- ENI secondary IP addressing
- Cross-AZ communication troubleshooting

### DevOps Practices
- Zero-downtime deployment strategies
- Comprehensive documentation
- Systematic troubleshooting methodology
- Following official documentation
- Automated state tracking

## ğŸ› Challenges & Solutions

### Challenge: Workers Unable to Join Cluster

**Problem:** Worker nodes in different availability zone couldn't reach master's Kubernetes API server.

**Investigation:**
```bash
# Workers could reach primary IP
ping 172.31.82.189  # âœ… Success

# But not the secondary IP used by K8s API
ping 172.31.89.68   # âŒ Failed
```

**Root Cause:** Secondary IP address was added at OS level but not registered with AWS ENI.

**Solution:**
```bash
aws ec2 assign-private-ip-addresses \
  --network-interface-id eni-054cab4e32bf4c5fc \
  --private-ip-addresses 172.31.89.68
```

**Key Learning:** Cloud provider networking requires proper API registration, not just OS-level configuration.

## ğŸ“Š Results & Metrics

| Metric | Value |
|--------|-------|
| **Total Upgrade Time** | ~90 minutes |
| **Application Downtime** | 0 seconds |
| **Pods Migrated** | 6 |
| **Failed Attempts** | 0 |
| **Rollbacks Required** | 0 |
| **Documentation Lines** | 735 |

### Final Verification
```
NAME               STATUS   ROLES           VERSION
master             Ready    control-plane   v1.30.14
ip-172-31-28-145   Ready    <none>          v1.30.14
ip-172-31-25-165   Ready    <none>          v1.30.14
```

## ğŸ“ Repository Structure
```
.
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ upgrade-session.log        # Complete terminal capture (735 lines)
â”œâ”€â”€ cluster-states/
â”‚   â”œâ”€â”€ master-upgrade.yaml        # Post-master upgrade state
â”‚   â”œâ”€â”€ final-state.yaml           # Final cluster state
â”‚   â””â”€â”€ complete-cluster-snapshot.yaml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ troubleshooting-aws-networking.md
â”‚   â””â”€â”€ upgrade-procedure.md
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ pre-upgrade.png
â”‚   â””â”€â”€ post-upgrade.png
â””â”€â”€ QUICK_START.md                 # Quick reference guide
```

## ğŸš€ Quick Start

### View Complete Upgrade Log
```bash
cat logs/upgrade-session-20251021-083000.log
```

### Examine Cluster States
```bash
# Pre-upgrade state
cat cluster-states/pre-upgrade.yaml

# Final state
cat cluster-states/final-state.yaml
```

### Apply ConfigMaps to Your Cluster
```bash
kubectl apply -f cluster-states/
```

## ğŸ› ï¸ Technologies Used

- **Kubernetes** 1.29 â†’ 1.30.14
- **kubeadm** - Cluster lifecycle management
- **AWS EC2** - Compute infrastructure
- **Calico** - Container networking (CNI)
- **Ubuntu** 22.04 LTS
- **kubectl** - Kubernetes CLI
- **containerd** - Container runtime

## ğŸ“š References & Resources

- [Official Kubernetes Upgrade Documentation](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)
- [Kubernetes 1.30 Release Notes](https://kubernetes.io/blog/2024/04/17/kubernetes-v1-30-release/)
- [AWS VPC Networking Guide](https://docs.aws.amazon.com/vpc/)

## ğŸ’¡ Key Takeaways

1. **Planning Prevents Issues** - Understanding upgrade paths and prerequisites is crucial
2. **Documentation is Essential** - Real-time logging captures invaluable troubleshooting information
3. **Cloud â‰  Traditional Infrastructure** - Cloud providers require proper resource registration through APIs
4. **Official Docs Work** - Following Kubernetes documentation ensures reliable upgrades
5. **Zero-Downtime is Achievable** - Proper use of cordon/drain enables seamless production upgrades

## ğŸ¤ Connect

**Ibrahim Cisse**  
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?logo=linkedin)](https://www.linkedin.com/in/ibraheemcisse)
[![Email](https://img.shields.io/badge/Email-Contact-red?logo=gmail)](ibrahimcisse@ioc-labs.com)

---

â­ **Star this repository if you found it helpful!**

ğŸ“ **Questions or suggestions?** Open an issue or reach out!

*This project demonstrates production-ready Kubernetes administration and cloud infrastructure management capabilities.*
